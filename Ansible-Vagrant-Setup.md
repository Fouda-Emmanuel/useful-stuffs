# Ansible Lab with Vagrant VMs Setup Documentation 

## ğŸ¯ Goal: Create a Local Ansible Lab That Mimics Real Cloud Setup

**Why?** - To learn Ansible without cloud costs, using Vagrant to simulate real servers in a safe environment.

---

## Step 1: Setting Up Virtual Machines

### What We Did:
Created 4 virtual machines using Vagrant:
- **control** (Ubuntu) - Where Ansible runs from
- **web01** (CentOS) - Web server 1  
- **web02** (CentOS) - Web server 2
- **db01** (CentOS) - Database server

### Why This Setup?
This mimics real-world infrastructure where one control machine manages multiple servers, just like in production environments.

### Vagrantfile Configuration:
```ruby
Vagrant.configure("2") do |config|
  # CONTROL NODE (Ubuntu)
  config.vm.define "control" do |control|
    control.vm.box = "ubuntu/jammy64"
    control.vm.hostname = "control"
    control.vm.network "private_network", ip: "192.168.56.10"
  end

  # WEB01 (CentOS)
  config.vm.define "web01" do |web01|
    web01.vm.box = "centos/stream9"
    web01.vm.hostname = "web01"
    web01.vm.network "private_network", ip: "192.168.56.11"
  end

  # WEB02 (CentOS)
  config.vm.define "web02" do |web02|
    web02.vm.box = "centos/stream9"
    web02.vm.hostname = "web02"
    web02.vm.network "private_network", ip: "192.168.56.12"
  end

  # DB01 (CentOS)
  config.vm.define "db01" do |db01|
    db01.vm.box = "centos/stream9"
    db01.vm.hostname = "db01"
    db01.vm.network "private_network", ip: "192.168.56.13"
  end
end
```

### Important: Network Configuration

**Understanding Private Network IPs:**
The IP addresses `192.168.56.10-13` allow your VMs to communicate with each other and your host machine in an isolated network.

**Checking Your Network Compatibility:**
```bash
VBoxManage list hostonlyifs
```

Look for the `IPAddress` line. If you see:
- `192.168.56.1` â†’ Use `192.168.56.10-13` âœ“
- `192.168.33.1` â†’ Use `192.168.33.10-13`
- `192.168.10.1` â†’ Use `192.168.10.10-13`

**Recommended: Use DHCP (Automatic IP Assignment)**
Replace static IP configuration with:
```ruby
config.vm.network "private_network", type: "dhcp"
```

Check assigned IPs after setup:
```bash
vagrant ssh control
ip addr show
```

### Commands to Deploy:
```bash
vagrant up          # Start all VMs
vagrant status      # Check VM status
```

---

## Step 2: Installing Ansible

### What We Did:
Installed Ansible only on the control node.

### Why?
Ansible is agentless - it runs commands from one central machine to manage others remotely.

### Installation Commands (run on control node):
```bash
sudo apt update
sudo apt install software-properties-common
sudo add-apt-repository --yes --update ppa:ansible/ansible
sudo apt install ansible
ansible --version   # Verify installation
```

---

## Step 3: Understanding SSH Keys

### What Are SSH Keys?
- **Private Key**: Your secret password (keep secure)
- **Public Key**: The matching lock (place on servers)

### In Our Lab:
Each VM has its own private key generated by Vagrant, similar to AWS .pem files.

### Key Locations:
```
.vagrant/machines/control/virtualbox/private_key
.vagrant/machines/web01/virtualbox/private_key  
.vagrant/machines/web02/virtualbox/private_key
.vagrant/machines/db01/virtualbox/private_key
```

---

## Step 4: Creating Ansible Inventory File

### What We Did:
Created a file telling Ansible about our servers and how to connect to them.

### First Attempt (Failed):
```bash
ansible web01 -m ping
# ERROR: provided hosts list is empty
```

### Problem:
Ansible couldn't find our inventory file.

### Solutions:
1. Specify inventory file manually:
```bash
ansible web01 -m ping -i inventory
```

2. Create ansible.cfg to set default inventory (recommended)

### Final Inventory File (inventory):
```yaml
all:
  children:
    webservers:
      hosts:
        web01:
          ansible_host: 192.168.56.11
          ansible_user: vagrant
          ansible_ssh_private_key_file: ./web01-key.pem
        web02:
          ansible_host: 192.168.56.12
          ansible_user: vagrant
          ansible_ssh_private_key_file: ./web02-key.pem
    dbservers:
      hosts:
        db01:
          ansible_host: 192.168.56.13
          ansible_user: vagrant
          ansible_ssh_private_key_file: ./db01-key.pem
```

---

## Step 5: Setting Up SSH Keys in Project

### What We Did:
Copied Vagrant's private keys to our project folder for better organization.

### Commands (from Windows host):
```bash
cat "C:/Users/DELL LATITUDE 7280/vagrant-vms/ansible-lab/.vagrant/machines/web01/virtualbox/private_key" > web01-key.pem
cat "C:/Users/DELL LATITUDE 7280/vagrant-vms/ansible-lab/.vagrant/machines/web02/virtualbox/private_key" > web02-key.pem  
cat "C:/Users/DELL LATITUDE 7280/vagrant-vms/ansible-lab/.vagrant/machines/db01/virtualbox/private_key" > db01-key.pem
```

### Critical Error: File Permissions Too Open
```bash
ansible web01 -m ping -i inventory
# ERROR: Permissions 0664 for './web01-key.pem' are too open
```

### Solution:
SSH requires private keys to be readable only by the owner for security:
```bash
chmod 400 *.pem
```

---

## Step 6: Solving Host Key Verification

### Problem: SSH Fingerprint Prompt
```
The authenticity of host '192.168.56.11' can't be established.
Are you sure you want to continue connecting (yes/no)?
```

### Why This Happens:
SSH security feature that breaks automation in lab environments.

### Solution: Disable host key checking in ansible.cfg

**Process:**
1. Backup original config:
```bash
sudo -i
cd /etc/ansible/
sudo mv /etc/ansible/ansible.cfg /etc/ansible/ansible.cfg_backup
```
2. Create ansible.cfg in /etc/ansible/ and set the `host_key_checking` :
```bash
sudo -i
cd /etc/ansible/
ansible-config init --disabled -t all > ansible.cfg
```
Then edit `/etc/ansible/ansible.cfg` and set:
```ini
[defaults]
host_key_checking = False
```

### What This Config Does:
- `host_key_checking = False` - Skip SSH confirmation prompts

---

## Step 7: Testing Everything Works

### Final Test Commands:
```bash
# Test individual servers
ansible web01 -m ping -i inventory
ansible web02 -m ping  -i inventory
ansible db01 -m ping -i inventory

# Test server groups
ansible webservers -m ping -i inventory
ansible dbservers -m ping -i inventory

# Test all servers
ansible all -m ping -i inventory
```

### âœ… Expected Success Output:
```
web01 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
```

---

## ğŸ“ Final Project Structure
```
ansible-lab/
â”œâ”€â”€ Vagrantfile          # VM definitions
â”œâ”€â”€ inventory            # Server list (YAML format)
â”œâ”€â”€ control-key.pem      # SSH key for control node
â”œâ”€â”€ web01-key.pem        # SSH key for web01
â”œâ”€â”€ web02-key.pem        # SSH key for web02
â””â”€â”€ db01-key.pem         # SSH key for db01
```

## ğŸ¯ Key Lessons Learned

1. **Ansible is agentless** - manages servers from one control machine
2. **Inventory file** defines which servers to manage and how to connect
3. **SSH keys require correct permissions** (`chmod 400` for security)
4. **Host key checking should be disabled** in lab environments
5. **Vagrant private keys** function like cloud .pem files
6. **Groups in inventory** help organize servers logically

## ğŸš€ You're Now Ready To:
- Write and execute Ansible playbooks
- Automate software installation across multiple servers
- Configure and manage infrastructure as code
- Develop real DevOps automation skills

This lab provides the same experience as working with real cloud servers, but completely free and locally contained!
